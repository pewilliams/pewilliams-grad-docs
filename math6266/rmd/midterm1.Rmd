---
title: 'Midterm 1: Math 6266'
author: "Peter Williams"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#### Section 1.1
*Exercise 1. Consider the linear regression model with mean zero, uncorrelated, heteroscedastic noise:*
\begin{equation}
  Y_i = X_i^{\intercal}\theta + \varepsilon_i,\ for\ i=1,..,n,\ E\varepsilon_i = 0,\  
  cov(\varepsilon_i, \varepsilon_j)=\left\{
  \begin{array}{@{}ll@{}}
    \sigma_i^2, & \text{if}\ i=j \\
    0, & i \neq j
  \end{array}\right.
\end{equation}
 
*Find expressions for the $LSE$ and response estimator in this model*

To set up the problem, take $W^{-1} = diag\{\sigma_1^2,...,\sigma_n^2\}$,  $W = diag\{\frac{1}{\sigma_1^2},...,\frac{1}{\sigma_n^2}\}, W^{1/2} = diag\{\sqrt{\frac{1}{\sigma_1^2}},...,\sqrt{\frac{1}{\sigma_n^2}}\}$, with $W^{\intercal} = W$, and $W^{1/2}W^{1/2} = W$, since they are diagonal matrices. Also we will use $w_i = \frac{1}{\sigma_i^2} = W_{ii}$.

Under heteroscedastic noise assumptions, we define the least squares estimator, denoted $\hat{\theta}$, as: 
$$\hat{\theta} = \underset{\theta}{argmin}\sum_{i=1}^n w_i (Y_i - X_i^{\intercal}\theta)^2  = \underset{\theta}{argmin}\sum_{i=1}^n (\sqrt{w_i}Y_i - \sqrt{w_i}X_i^{\intercal}\theta)^2 = \underset{\theta}{argmin} ||W^{1/2}Y - W^{1/2}X^{\intercal}\theta||^2$$
$$G(\theta) = ||W^{1/2}Y - W^{1/2}X^{\intercal}\theta||^2 =  (W^{1/2}Y - W^{1/2}X^{\intercal}\theta)^{\intercal}(W^{1/2}Y - W^{1/2}X^{\intercal}\theta) = Y^{\intercal}WY - 2\theta^{\intercal}XWY + \theta^{\intercal}XWX^{\intercal}\theta$$ with gradient, $$\nabla G(\theta)  = -2XWY + 2XWX^{\intercal}\theta$$ 
Setting this expression equal to zero leads to estimator $\hat{\theta} = (XWX^{\intercal})^{-1}XWY$, which leads to response estimator $\hat{Y} = X^{\intercal}\hat{\theta} = X^{\intercal}(XWX^{\intercal})^{-1}XWY$.

*Exercise 2.  Assume that $\varepsilon_i \sim N(0, \sigma_i^2)$ in the previous problem. What is known about the distribution of $\hat{\theta}$ and $\hat{Y}$?*

For $\hat{\theta}$, we have, 
$$E[{\hat{\theta}}] = E[(XWX^{\intercal})^{-1}XWY] =  E[(XWX^{\intercal})^{-1}XW(X^{\intercal}\theta^{*} + \varepsilon)] = E[\theta^{*}] + E[(XWX^{\intercal})^{-1}XW\varepsilon] = \theta^{*}$$ indicating that $\hat{\theta}$ is unbiased. Further $\hat{\theta}$ is normally distributed, since is a linear transformation of $\varepsilon \sim N(0,W^{-1})$. Further we have,
$$Var({\hat{\theta}}) = Var((XWX^{\intercal})^{-1}XWY) = Var((XWX^{\intercal})^{-1}XW(X^{\intercal}\theta^{*} + \varepsilon)) = Var((XWX^{\intercal})^{-1}XW\varepsilon)) =...$$ 
$$ = (XWX^{\intercal})^{-1}XW Var(\varepsilon) W^{\intercal}X^{\intercal}(XWX^{\intercal})^{-1} = (XWX^{\intercal})^{-1}XWX^{\intercal}(XWX^{\intercal})^{-1} = (XWX^{\intercal})^{-1} = Var(\hat{\theta})$$ 

For $\hat{Y}$ we have,
$$E[\hat{Y}] = E[X^{\intercal}(XWX^{\intercal})^{-1}XWY] = E[X^{\intercal}(XWX^{\intercal})^{-1}XW(X^{\intercal}\theta^* + \varepsilon)] = E[X^{\intercal}\theta^* + X^{\intercal}(XWX^{\intercal})^{-1}XW\varepsilon] = E[X^{\intercal}\theta^*]= Y$$
and, 
$$Var[\hat{Y}] = Var[X^{\intercal}(XWX^{\intercal})^{-1}XWY] = Var[X^{\intercal}(XWX^{\intercal})^{-1}XW(X^{\intercal}\theta^* + \varepsilon)] = Var[X^{\intercal}\theta^* + X^{\intercal}(XWX^{\intercal})^{-1}XW\varepsilon] =\ ...$$
$$... = Var[X^{\intercal}(XWX^{\intercal})^{-1}XW\varepsilon] = X^{\intercal}(XWX^{\intercal})^{-1}XW\ Var(\varepsilon)\ W^{\intercal}X^{\intercal}(XWX^{\intercal})^{-1}X =...$$
$$=X^{\intercal}(XWX^{\intercal})^{-1}XWX^{\intercal}(XWX^{\intercal})^{-1}X = X^{\intercal}(XWX^{\intercal})^{-1}X = Var[\hat{Y}]$$
\newline

*Now suppose additionally that $\sigma_i^2 \equiv \sigma^2 > 0$.* 
*What can be said about distribution of the estimator $\hat{\sigma^2}$?*

With $\sigma_i^2 \equiv \sigma^2 > 0$, we have $\hat{\sigma^2} = \frac{||Y-X^{\intercal}\hat{\theta}||^2}{n-p} = \frac{||\hat{\varepsilon}||^2}{n-p}$. Further denote, $||\hat{\varepsilon}|| = ||Y-\hat{Y}|| = ||Y - \Pi Y|| = ||(I_n - \Pi)Y||$, also noting that $(I_n - \Pi)X^{\intercal} = X^{\intercal} - \Pi X^{\intercal} = X^{\intercal} - X^{\intercal}(XX^{\intercal})^{-1}XX^{\intercal} = X^{\intercal} - X^{\intercal} = 0$.  

Then we have, $$(n-p)E[\hat{\sigma^2}] = E||Y-X^{\intercal}\hat{\theta}||^2 = E||\hat{\varepsilon}||^2 = E[tr(\hat{\varepsilon}\hat{\varepsilon}^{\intercal})] = E [tr((I_n - \Pi)YY^{\intercal}(I_n - \Pi))] = ...$$
$$= E [tr((I_n - \Pi)(X^{\intercal}\theta^* + \varepsilon)(X^{\intercal}\theta^* + \varepsilon)^{\intercal}(I_n - \Pi))] = E [tr((I_n - \Pi)\varepsilon\varepsilon^{\intercal}(I_n - \Pi))] = tr((I_n - \Pi)E[\varepsilon\varepsilon^{\intercal}]) = ...$$  
Using the cylic property of the trace operator, the property that $(I_n - \Pi)(I_n - \Pi) = (I_n - \Pi)$, and the expectation $E[\varepsilon\varepsilon^{\intercal}] = \sigma^2I_n$, leading to $$...=\sigma^2tr(I_n - \Pi) = \sigma^2(n-p) = (n-p) E[\hat{\sigma}^2]$$
Looking further at the distribution of $||Y-X^{\intercal}\hat{\theta}||^2 = \hat{\varepsilon}^{\intercal}\hat{\varepsilon}$, we have
$$\hat{\varepsilon}^{\intercal}\hat{\varepsilon} = ((I_n-\Pi)Y)^{\intercal}((I_n-\Pi)Y) = Y^\intercal(I_n-\Pi)Y = (X^{\intercal}\theta^* + \varepsilon)^{\intercal}(I_n - \Pi)(X^{\intercal}\theta^* + \varepsilon) = \varepsilon^{\intercal}(I_n - \Pi)\varepsilon$$ 

Since we know that $\varepsilon \sim N(0, \sigma^2 I_n)$, and further $\frac{\varepsilon^{\intercal}\varepsilon}{\sigma^2} \sim \chi^2(n)$, $(\frac{\varepsilon}{\sigma})^{\intercal}(I_n-\Pi)(\frac{\varepsilon}{\sigma}) \sim \chi^2(n-p)$, since we know from earlier that $(I_n - \Pi)$, is idempotent, with rank equal to $tr(I_n-\Pi) = tr(I_n) - tr(\Pi) = n - p$. 

#### Section 1.3
*Exercise 4. Let $A \in R^{n\times n}$ be a matrix (corresponding to a linear map in $R^n$). Show that $A$ preserves length for all $x \in R^n$ iff it preserves the inner product. I.e. one needs to show the following:*

*$||Ax|| = ||x||\  \forall\ x \in R^n \iff (Ax)^{\intercal}(Ay)\  \forall\  x,y \in R^n$.*

Take, 
$$||x|| = \sqrt{x \cdot x} = \sqrt{x^\intercal x} \implies ||Ax|| = \sqrt{Ax \cdot Ax} = \sqrt{x^{\intercal}A^{\intercal}Ax} \implies$$,
$$ A^\intercal A = I_n = A^{-1},\ A^\intercal = A^{-1}, ||Ax|| = ||x||$$
this implies $A$ is an orthogonal matrix, and further, 
$$(Ax)^\intercal (Ay)  = ||AxAy||^2 = x^\intercal A^\intercal A y = x^\intercal y = ||xy||^2$$

*Exercise 5. (a) Let $x_0 \in R^n$ be some fixed vector, find a projection map on the subspace $span(x_0)$. Compare your result with matrix $\Pi$ (from section 1.3) for the case of $p=1$.*
 
Let $x = span(x_0) = span(x_1,x_2,..,x_n)$, denote the subspace of interest, and $x_1,x_2,...$ are basis vectors and $y = (y_1,y_2, ..., y_n)^{\intercal}$. The projection map is,
$$Proj_{x}(y) = \frac{<y \cdot x>}{<y \cdot y>} x = \sum_{i=1}^{n}\frac{<y_i \cdot\ x_i>}{<y_i \cdot y_i>}x_i$$
For the case $p=1$, and $\Pi = X^{\intercal}(XX^{\intercal})^{-1}X, X^{\intercal} \in R^n$, we have,
$$\Pi y  = \hat{y} = X^{\intercal}(XX^{\intercal})^{-1}Xy = X^{\intercal} \frac{Xy}{XX^{\intercal}} = \frac{\sum_i^nx_iy_i}{\sum_i^nx_i^2} (x_1,x_2,...,x_n)^{\intercal} = \frac{<X \cdot y>}{<y \cdot y>} X^{\intercal} =Proj_{X}(y)$$ 

*(b) Prove part 3) of Lemma 1.1 for an arbitrary orthogonal projection in $R^n$. Show $\forall h \in R^n$, $||h||^2 = ||\Pi h||^2 + ||h - \Pi h ||^2$.* 

Using the fact that $(I_n-\Pi)^{\intercal}(I_n-\Pi) = I_n - 2\Pi + \Pi  = I_n - \Pi$, we have,
$$||h||^2 = ||\Pi h||^2 + ||h - \Pi h ||^2 = h^{\intercal}\Pi^{\intercal}\Pi h  + h^{\intercal}(I_n - \Pi)^{\intercal}(I_n - \Pi)h = h^{\intercal} \Pi h + h^{\intercal}(I_n - \Pi)h = h^{\intercal}I_n h  + h^{\intercal} \Pi h - h^{\intercal} \Pi h = ||h||^2$$

#### Section 2.1

*Exercise 8.* 
*Let $X \sim N(0,I_n)$, $Q = X^{\intercal} X$.* 
*Suppose that $Q$ is decomposed into the sum of two quadratic forms: $Q = Q1 + Q2$, where $Qi = X^{\intercal}A_iX$, $i = 1, 2$ for some symmetric matrices $A1,A2$ with $rank(A1) = n1$ and $rank(A2) = n2$. Show that if $n1 + n2 = n$, then $Q1$ and $Q2$ are independent and $Q_i \sim \chi^2(n_i) for\  i = 1,2$.*
 
First note that $X^{\intercal}X \sim \chi^2(n)$, since $X^{\intercal}X = \sum_{i=1}^n x_i^2$, which is the sum of iid squared normal random variables with variance $1$.

Since $A1$ is a symmetric matrix, we can diagonalize it, $A_1 = U^{\intercal}\Lambda U$. We know the rank of $A_1$ is $n_1$. This implies that $U^{\intercal}A_1U = \Lambda = diag\{\Lambda_1,...,\Lambda_{n_1},...,\Lambda_{n}\}$, has $n_1$ non-zero, positive eigenvalues, and $n_2$ eigenvalues that equal zero. 

Using the orthogonal matrix $U$ from the decomposition of $A_1$, we set $X = UY$, so that $X^{\intercal}X = Y^{\intercal}U^{\intercal}UY = Y^{\intercal} I_n Y = Y^{\intercal}Y$. So $Q = X^{\intercal}X = Y^{\intercal}Y = \sum_{i=1}^{n}Y_i^2$. 

We can write 
$$Q = Q_1 + Q_2 = \sum_{i=1}^{n}Y_i^2 = Y^\intercal U^\intercal A_1UY + Y^\intercal U^\intercal A_2UY = Y^\intercal \Lambda Y + Y^\intercal U^\intercal A_2UY = \sum_{i=1}^{n}\Lambda_i Y_i^2 + Y^\intercal U^\intercal A_2UY$$ 
Since only $n_1$ eigenvalues in $\Lambda$ are non-zero, we have 
$$Q =\sum_{i=1}^{n_1}\Lambda_iY_i^2 + \sum_{i=n_1+1}^{n}\Lambda_iY_i^2 +  Y^\intercal U^\intercal A_2UY = Q =\sum_{i=1}^{n_1}\Lambda_i Y_i^2 + Y^\intercal U^\intercal A_2UY$$, 

if we organize $\Lambda$ in way such that the positive eigenvalues on the diagonal are present in the first $n_1$ diagonal elements. So we have $Q_1 = \sum_{i=1}^{n_1}\Lambda_iY_i^2$ 

To solve for $Q_2 = X^{\intercal}X = Y^{\intercal}U^{\intercal}A_2 UY$, from above we have 
$$Y^{\intercal}U^{\intercal}A_2 UY = Q - Q_1 = Q - \sum_{i=1}^{n_1}\Lambda_iY_i^2 = \sum_{i=1}^{n_1}Y_i^2 + \sum_{i=n_1+1}^{n}Y_i^2  -  \sum_{i=1}^{n_1}\Lambda_iY_i^2 = \sum_{i=1}^{n_1}(1 - \Lambda_i)Y_i^2 +  \sum_{i=n_1 + 1}^{n}Y_i^2$$
We know the rank of $A_2$ is $n_2 = n - n_1$. So the term $\sum_{i=1}^{n_1}(1 - \Lambda_i)Y_i^2$ must equal zero, implying that $\Lambda_1 = \Lambda_2 = ... = \Lambda_{n_1} = 1$. This also implies $Q = Q1 + Q2 = \sum_{i=1 + 1}^{n_1}Y_i^2  + \sum_{i=n_1 + 1}^{n}Y_i^2$. 

Since each squared element $Y_i^2 = X_i^2 \sim \chi^2(1)$ in $Q$ only occurs once in the summand, we can say that and $Q1 = \sum_{i=1}^{n_1} Y_i^2 \sim \chi^2(n_1)$, and  $Q2 = \sum_{i=n_1 + 1}^{n} Y_i^2 \sim \chi^2(n_2)$. 
